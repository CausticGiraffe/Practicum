{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Cervix Cancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type_1\n",
      "Type_2\n",
      "Type_3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import cv2\n",
    "import skimage.io as io\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls \n",
    "py.init_notebook_mode(connected=True)\n",
    "import random \n",
    "import shutil\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"/home/kray/Practicum/train\"]).decode(\"utf8\"))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249 photos of cervix type Type_1\n",
      "781 photos of cervix type Type_2\n",
      "450 photos of cervix type Type_3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAEVCAYAAAAmWLk4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nz9Fnkm82/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273c81c9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of images for each cervix type.\n",
    "sub_folders = check_output([\"ls\", \"/home/kray/Practicum/train\"]).decode(\"utf8\").strip().split('\\n')\n",
    "count_dict = {}\n",
    "for sub_folder in sub_folders:\n",
    "    num_of_files = len(check_output([\"ls\", \"/home/kray/Practicum/train/\"+sub_folder]).decode(\"utf8\").strip().split('\\n'))\n",
    "    print(\"{0} photos of cervix type {1}\".format(num_of_files, sub_folder))\n",
    "                            \n",
    "    count_dict[sub_folder] = num_of_files\n",
    "                            \n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(list(count_dict.keys()), list(count_dict.values()), alpha = 0.8)\n",
    "plt.xlabel('Cervix types', fontsize = 11)\n",
    "plt.ylabel('Number of images in train', fontsize = 11)\n",
    "plt.title('train dataset')\n",
    "                            \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images present: 512\n"
     ]
    }
   ],
   "source": [
    "num_test_files = len(check_output([\"ls\", \"/home/kray/Practicum/test/\"]).decode(\"utf8\").strip().split('\\n'))\n",
    "print(\"Number of test images present:\", num_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_path = \"/home/kray/Practicum/train/\"\n",
    "sub_folders = check_output([\"ls\", train_path]).decode(\"utf8\").strip().split('\\n')\n",
    "different_file_sizes = {}\n",
    "for sub_folder in sub_folders:\n",
    "    file_names = check_output([\"ls\", train_path+sub_folder]).decode(\"utf8\").strip().split('\\n')\n",
    "    for file_name in file_names:\n",
    "        im_array = imread(train_path+sub_folder+\"/\"+file_name)\n",
    "        size = \"_\".join(map(str,list(im_array.shape)))\n",
    "        different_file_sizes[size] = different_file_sizes.get(size,0) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2448_3264_3': 29,\n",
      " '3088_4128_3': 1,\n",
      " '3096_4128_3': 14,\n",
      " '3264_2448_3': 702,\n",
      " '4128_2322_3': 17,\n",
      " '4128_3096_3': 677,\n",
      " '4160_3120_3': 34,\n",
      " '640_480_3': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f273c4c2eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "#sns.barplot(list(different_file_sizes.values()), list(different_file_sizes.keys()), alpha=0.8)\n",
    "import pprint\n",
    "pprint.pprint(different_file_sizes)\n",
    "#plt.ylabel('Image size', fontsize = 11)\n",
    "#plt.xlabel('Number of images in train', fontsize = 11)\n",
    "#plt.title(\"Image sizes present in train dataset\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 1036\n",
      "Validation images 444\n"
     ]
    }
   ],
   "source": [
    "#Split data into training and validation sets\n",
    "allFiles = glob.glob('/home/kray/Practicum/train/*/*')\n",
    "\n",
    "split_point = int(round(0.7*len(allFiles)))\n",
    "random.shuffle(allFiles)\n",
    "\n",
    "train_list = allFiles[:split_point]\n",
    "valid_list = allFiles[split_point:]\n",
    "print('Train images: {}'.format(len(train_list)))\n",
    "print('Validation images {}'. format(len(valid_list)))\n",
    "\n",
    "#Divy them into their own directories\n",
    "train_data_path = '/home/kray/Practicum/train_data'\n",
    "validation_data_path = '/home/kray/Practicum/valid_data'\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "if not os.path.exists(validation_data_path):\n",
    "    os.makedirs(validation_data_path)\n",
    "\n",
    "for fpath in train_list:\n",
    "    basename = fpath.split('/')[-2:]\n",
    "    dest = '/'.join([train_data_path] +basename)\n",
    "    if not os.path.exists('/'.join(dest.split('/')[:-1])):\n",
    "        os.makedirs('/'.join(dest.split('/')[:-1]))\n",
    "    shutil.copyfile(fpath, '/'.join([train_data_path] +basename))\n",
    "for fpath in valid_list:\n",
    "    basename = fpath.split('/')[-2:]\n",
    "    dest = '/'.join([validation_data_path] +basename)\n",
    "    if not os.path.exists('/'.join(dest.split('/')[:-1])):\n",
    "        os.makedirs('/'.join(dest.split('/')[:-1]))\n",
    "    shutil.copyfile(fpath, '/'.join([validation_data_path] +basename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dimensions of images\n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1036 images belonging to 3 classes.\n",
      "Found 444 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 224] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./224)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_path,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_path,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Small conv net\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.compile(loss='binary_crossentropy',\n",
    "              #optimizer='rmsprop',\n",
    "              #metrics=['accuracy'])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.01, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "nb_epoch = 1\n",
    "nb_train_samples = 1036\n",
    "nb_validation_samples = 444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: /home/kray/Practicum/train_data...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-a14a7dfec3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#nb_val_samples=nb_validation_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kray/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/kray/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kray/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    979\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m    982\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m    983\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kray/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m     72\u001b[0m                             \u001b[0;34m': data should be a Numpy array, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                             \u001b[0;34m'or list/dict of Numpy arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                             'Found: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# case: model expects multiple inputs but only received\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: /home/kray/Practicum/train_data..."
     ]
    }
   ],
   "source": [
    "#model.fit_generator(\n",
    "        #train_generator,\n",
    "        #samples_per_epoch = nb_train_samples,\n",
    "        #nb_epoch=nb_epoch,\n",
    "        #validation_data = validation_generator,\n",
    "        #nb_val_samples=nb_validation_samples)\n",
    "        \n",
    "model.fit(train_data_path, validation_data_path, batch_size=32, nb_epoch=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/basic_cnn_1_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluating on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing loss and accuracy \n",
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data augmentation for improving model by applying random transformation to the train set. \n",
    "#reduces overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an ensembe with pre-trained models: VGG16, ResNet_V2, and Inception_V3, then fine tune the top layers of the pre-trained networks. \n",
    "https://gihub.com/tensorflow/models/blob/master/slim/README.md#Pretrained\n",
    "\n",
    "2. Experiment with more k-folds on the training images\n",
    "\n",
    "3. Explore bounding box annotations \n",
    "\n",
    "4. Train and test the improved model on the additional images and second relase images\n",
    "\n",
    "5. Incorrporate additional visualizations"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
